import asyncio
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
import uvicorn
from langchain_core.messages import HumanMessage
import json

import warnings
# Pydantic í•„ë“œ ì´ë¦„ ì¶©ëŒ ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")

from config import MCP_SERVERS, logger
from mcp_client import MCPClient
from agent_graph import create_agent_app

from contextlib import asynccontextmanager

# FastAPI ì•±ì˜ ìƒëª…ì£¼ê¸°(Lifecycle) ê´€ë¦¬
@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ê¸°ë™ ì‹œ ì´ˆê¸°í™” ë° ì¢…ë£Œ ì‹œ ì •ë¦¬ ë¡œì§"""
    global agent_app, mcp_clients
    
    logger.info("ğŸš€ [System] FastAPI ê¸°ë°˜ MCP Agent ê¸°ë™ ì‹œì‘...")
    all_tools = []
    
    # 1. ê¸°ë™ ì‹œ: MCP ì„œë²„ ì—°ê²° ë° ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    for server_conf in MCP_SERVERS:
        client = MCPClient(server_conf["name"], server_conf["url"])
        try:
            await client.connect()
            mcp_clients.append(client)
            all_tools.extend(client.tools)
        except Exception as e:
            logger.error(f"MCP Connection failed ({server_conf['name']}): {e}")
            
    if not mcp_clients:
        logger.warning("âŒ ì—°ê²°ëœ ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤. (ë„êµ¬ ì—†ì´ ì´ˆê¸°í™”ë©ë‹ˆë‹¤)")
    else:
        logger.info(f"âœ¨ ì´ {len(mcp_clients)}ê°œ ì„œë²„ ì—°ê²° ì™„ë£Œ. (ë„êµ¬ {len(all_tools)}ê°œ ì‚¬ìš© ê°€ëŠ¥)")
        
    # ì—ì´ì „íŠ¸ ì•± ìƒì„±
    agent_app = create_agent_app(all_tools)
    logger.info("âœ… API Server: Agent initialized with tools.")
    
    yield  # ì„œë²„ ì‹¤í–‰ ì¤‘ (ì´ ì‹œì ì— ìš”ì²­ì„ ë°›ìŠµë‹ˆë‹¤)
    
    # 2. ì¢…ë£Œ ì‹œ: MCP ì—°ê²° ì •ë¦¬
    logger.info("ğŸ§¹ ì—°ê²° ì¢…ë£Œ ì¤‘...")
    for client in mcp_clients:
        await client.cleanup()
    logger.info("ğŸ‘‹ Bye!")

# FastAPI ì•± ìƒì„±
app = FastAPI(title="K8s MCP Agent API", lifespan=lifespan)

# ì „ì—­ ë³€ìˆ˜ë¡œ ì—ì´ì „íŠ¸ ì•±ê³¼ í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬
agent_app = None
mcp_clients = []

# ========================================================
# ìì²´ Webì„ ìœ„í•œ ì¼ë°˜ API ì—”ë“œí¬ì¸íŠ¸
# ========================================================
@app.post("/api/chat")
async def chat_endpoint(request: Request):
    """ì¼ë°˜ì ì¸ ìì²´ ê°œë°œ ì›¹í˜ì´ì§€ì—ì„œ í˜¸ì¶œí•˜ê¸° ì‰¬ìš´ ëª¨ë“œ"""
    data = await request.json()
    user_input = data.get("message", "")
    
    logger.info(f"User > {user_input}")
    logger.debug("--- ğŸ”„ ì²˜ë¦¬ ì¤‘... ---")
    
    # LangGraph ì‹¤í–‰ ë° ìµœì¢… ê²°ê³¼ë§Œ ë°˜í™˜ (ìŠ¤íŠ¸ë¦¬ë°ì´ ì•„ë‹ ê²½ìš°)
    inputs = {"messages": [HumanMessage(content=user_input)]}
    result = await agent_app.ainvoke(inputs)
    
    # ê²°ê³¼ íŒŒì‹±í•˜ì—¬ ë°˜í™˜
    final_message = result["messages"][-1].content
    return {"reply": final_message}

# ========================================================
# OpenWebUI ì—°ë™ì„ ìœ„í•œ OpenAI í˜¸í™˜ API (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
# ========================================================
@app.post("/v1/chat/completions")
async def openai_compatible_endpoint(request: Request):
    """OpenWebUI ë“± OpenAI ê·œê²©ì„ ìš”êµ¬í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸ë¥¼ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸"""
    data = await request.json()
    
    # messages ë°°ì—´ì—ì„œ ë§ˆì§€ë§‰ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì¶”ì¶œ
    messages = data.get("messages", [])
    user_input = messages[-1]["content"] if messages else ""
    model_name = data.get("model", "qwen-k8s-agent")
    
    logger.info(f"[OpenWebUI] User > {user_input}")

    async def stream_generator():
        inputs = {"messages": [HumanMessage(content=user_input)]}
        from config import stream_queue
        
        # ë‚´ë¶€ ì§„í–‰ ìƒí™©ì„ OpenWebUIì—ë„ ë³´ì—¬ì£¼ê¸° ìœ„í•œ í—¬í¼ í•¨ìˆ˜
        def make_chunk(text):
            chunk = {
                "id": "chatcmpl-123",
                "object": "chat.completion.chunk",
                "model": model_name,
                "choices": [{"index": 0, "delta": {"content": text}, "finish_reason": None}]
            }
            return f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
        
        # OpenWebUI í˜¸í™˜ì„±ì„ ìœ„í•œ "ë‹¨ì¼ Think Block" ì „ì†¡ í—¬í¼
        # ì—¬ëŸ¬ ë²ˆ ì—´ê³  ë‹«ìœ¼ë©´ ë Œë”ëŸ¬ì— ì‹¬í•œ ë ‰ì´ ê±¸ë¦¬ë¯€ë¡œ, í•œ ë²ˆë§Œ ì—´ê³  ë‚´ë¶€ì—ì„œ ì¤„ë°”ê¿ˆì„ í†µí•´ ì¶”ê°€í•©ë‹ˆë‹¤.
        
        # --- ë³µí•© ìŠ¤íŠ¸ë¦¬ë° ë¡œì§ ---
        # 1. LangGraphì˜ astream ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼
        # 2. ë°±ê·¸ë¼ìš´ë“œ Workerì˜ ì§„í–‰ ìƒíƒœë¥¼ ë‹´ëŠ” stream_queue 
        
        import asyncio
        queue_task = None
        graph_task = None
        
        async def run_graph():
            async for event in agent_app.astream(inputs):
                for key, value in event.items():
                    if key == "router":
                        await stream_queue.put("EVENT:ğŸ”„ `[System]` ë¼ìš°í„° ëª¨ë“œ ê²°ì • ì¤‘...")
                    elif key == "orchestrator":
                        await stream_queue.put("EVENT:ğŸ“‹ `[System]` ì‘ì—… ê³„íš ìˆ˜ë¦½ ì¤‘...")
                    elif key == "workers":
                        results = value.get("worker_results", [])
                        await stream_queue.put(f"EVENT:ğŸ‘· `[System]` {len(results)}ê°œ ë³‘ë ¬ ì‘ì—… ì‹¤í–‰ ì™„ë£Œ.")
                    elif key == "synthesizer":
                        pass
                    elif key == "simple_agent":
                        msg = value["messages"][-1].content
                        await stream_queue.put(f"FINAL:{msg}")
            
            await stream_queue.put("EOF")

        graph_task = asyncio.create_task(run_graph())
        
        has_started_thinking = False
        has_finished_thinking = False

        while True:
            # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€(ìƒˆë¡œê³ ì¹¨, ì¤‘ì§€ë²„íŠ¼, íƒ€ì„ì•„ì›ƒ ì¬ì‹œë„ ë“±) ê°ì§€
            if await request.is_disconnected():
                logger.warning("âš ï¸ [API] í´ë¼ì´ì–¸íŠ¸(OpenWebUI) ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                if graph_task and not graph_task.done():
                    graph_task.cancel()
                break

            try:
                # 5ì´ˆë§ˆë‹¤ íƒ€ì„ì•„ì›ƒì„ ë°œìƒì‹œì¼œ ë¹ˆ Ping(Keep-alive)ì„ ë³´ëƒ…ë‹ˆë‹¤.
                msg = await asyncio.wait_for(stream_queue.get(), timeout=5.0)
            except asyncio.TimeoutError:
                # OpenWebUIë‚˜ Proxyê°€ ì—°ê²°ì„ ëŠê±°ë‚˜ ì¬ì‹œë„(Retry)í•˜ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´
                # SSE í‘œì¤€ ì£¼ì„(:)ì„ í™œìš©í•œ Keep-Alive í•‘ ì „ì†¡
                yield ": keep-alive\n\n"
                continue

            if msg == "EOF":
                if has_started_thinking and not has_finished_thinking:
                    yield make_chunk("\n</think>\n\n")
                break
                
            elif msg.startswith("EVENT:"):
                text = msg.replace("EVENT:", "", 1)
                if not has_started_thinking:
                    yield make_chunk("<think>\n" + text + "\n")
                    has_started_thinking = True
                else:
                    yield make_chunk(text + "\n")
                    
            elif msg.startswith("TOKEN:"):
                # ì§„ì§œ ëª¨ë¸ì˜ ë‹µë³€ í† í°ì´ ì‹œì‘ë˜ê¸° ì§ì „ì— </think> ë¡œ ë‹«ìŠµë‹ˆë‹¤.
                if has_started_thinking and not has_finished_thinking:
                    yield make_chunk("\n</think>\n\n")
                    has_finished_thinking = True
                
                # ìŠ¤íŠ¸ë¦¬ë° í† í° (ì§„ì§œ ë‹µë³€)
                yield make_chunk(msg.replace("TOKEN:", "", 1))
                
            elif msg.startswith("FINAL:"):
                if has_started_thinking and not has_finished_thinking:
                    yield make_chunk("\n</think>\n\n")
                    has_finished_thinking = True
                    
                # ìµœì¢… ê²°ê³¼ ë¦¬í„´ (ë‹¨ìˆœ ì—ì´ì „íŠ¸ ì „ìš©)
                yield make_chunk(msg.replace("FINAL:", "", 1))
                
            else:
                # ğŸˆ ì„œë¸Œ ì—ì´ì „íŠ¸ ìš”ì•½ ì§„í–‰ ìƒí™© (â³ running for ...s) ì¶œë ¥
                if not has_started_thinking:
                    yield make_chunk("<think>\n" + msg + "\n")
                    has_started_thinking = True
                else:
                    yield make_chunk(msg + "\n")
                
        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ
        end_chunk = {
            "id": "chatcmpl-123",
            "object": "chat.completion.chunk",
            "model": model_name,
            "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]
        }
        yield f"data: {json.dumps(end_chunk, ensure_ascii=False)}\n\n"
        yield "data: [DONE]\n\n"

    return StreamingResponse(
        stream_generator(), 
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no" # Nginx/K8s Ingressì˜ SSE ë²„í¼ë§ ê°•ì œ ë¹„í™œì„±í™”
        }
    )

if __name__ == "__main__":
    uvicorn.run("api_server:app", host="0.0.0.0", port=8000, reload=True)
