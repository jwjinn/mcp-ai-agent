import asyncio
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
import uvicorn
from langchain_core.messages import HumanMessage
import json

import warnings
# Pydantic í•„ë“œ ì´ë¦„ ì¶©ëŒ ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")

from config import MCP_SERVERS, logger
from mcp_client import MCPClient
from agent_graph import create_agent_app

from contextlib import asynccontextmanager

# FastAPI ì•±ì˜ ìƒëª…ì£¼ê¸°(Lifecycle) ê´€ë¦¬
@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ê¸°ë™ ì‹œ ì´ˆê¸°í™” ë° ì¢…ë£Œ ì‹œ ì •ë¦¬ ë¡œì§"""
    global agent_app, mcp_clients
    
    logger.info("ğŸš€ [System] FastAPI ê¸°ë°˜ MCP Agent ê¸°ë™ ì‹œì‘...")
    all_tools = []
    
    # 1. ê¸°ë™ ì‹œ: MCP ì„œë²„ ì—°ê²° ë° ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    for server_conf in MCP_SERVERS:
        client = MCPClient(server_conf["name"], server_conf["url"])
        try:
            await client.connect()
            mcp_clients.append(client)
            all_tools.extend(client.tools)
        except Exception as e:
            logger.error(f"MCP Connection failed ({server_conf['name']}): {e}")
            
    if not mcp_clients:
        logger.warning("âŒ ì—°ê²°ëœ ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤. (ë„êµ¬ ì—†ì´ ì´ˆê¸°í™”ë©ë‹ˆë‹¤)")
    else:
        logger.info(f"âœ¨ ì´ {len(mcp_clients)}ê°œ ì„œë²„ ì—°ê²° ì™„ë£Œ. (ë„êµ¬ {len(all_tools)}ê°œ ì‚¬ìš© ê°€ëŠ¥)")
        
    # ì—ì´ì „íŠ¸ ì•± ìƒì„±
    agent_app = create_agent_app(all_tools)
    logger.info("âœ… API Server: Agent initialized with tools.")
    
    yield  # ì„œë²„ ì‹¤í–‰ ì¤‘ (ì´ ì‹œì ì— ìš”ì²­ì„ ë°›ìŠµë‹ˆë‹¤)
    
    # 2. ì¢…ë£Œ ì‹œ: MCP ì—°ê²° ì •ë¦¬
    logger.info("ğŸ§¹ ì—°ê²° ì¢…ë£Œ ì¤‘...")
    for client in mcp_clients:
        await client.cleanup()
    logger.info("ğŸ‘‹ Bye!")

# FastAPI ì•± ìƒì„±
app = FastAPI(title="K8s MCP Agent API", lifespan=lifespan)

# ì „ì—­ ë³€ìˆ˜ë¡œ ì—ì´ì „íŠ¸ ì•±ê³¼ í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬
agent_app = None
mcp_clients = []

# ========================================================
# ìì²´ Webì„ ìœ„í•œ ì¼ë°˜ API ì—”ë“œí¬ì¸íŠ¸
# ========================================================
@app.post("/api/chat")
async def chat_endpoint(request: Request):
    """ì¼ë°˜ì ì¸ ìì²´ ê°œë°œ ì›¹í˜ì´ì§€ì—ì„œ í˜¸ì¶œí•˜ê¸° ì‰¬ìš´ ëª¨ë“œ"""
    data = await request.json()
    user_input = data.get("message", "")
    
    logger.info(f"User > {user_input}")
    logger.debug("--- ğŸ”„ ì²˜ë¦¬ ì¤‘... ---")
    
    # LangGraph ì‹¤í–‰ ë° ìµœì¢… ê²°ê³¼ë§Œ ë°˜í™˜ (ìŠ¤íŠ¸ë¦¬ë°ì´ ì•„ë‹ ê²½ìš°)
    inputs = {"messages": [HumanMessage(content=user_input)]}
    result = await agent_app.ainvoke(inputs)
    
    # ê²°ê³¼ íŒŒì‹±í•˜ì—¬ ë°˜í™˜
    final_message = result["messages"][-1].content
    return {"reply": final_message}

# ========================================================
# ì»¤ìŠ¤í…€ React Flow ëŒ€ì‹œë³´ë“œ ì—°ë™ì„ ìœ„í•œ ì „ìš© SSE ìŠ¤íŠ¸ë¦¬ë°
# ========================================================
@app.post("/api/stream_chat")
async def react_flow_stream_endpoint(request: Request):
    """React Flow ê¸°ë°˜ì˜ 2D ì‹œê°í™” ëŒ€ì‹œë³´ë“œì™€ í†µì‹ í•˜ê¸° ìœ„í•œ Vercel AI SDK Data Stream í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸"""
    data = await request.json()
    messages = data.get("messages", [])
    user_input = messages[-1]["content"] if messages else data.get("message", "")
    
    logger.info(f"[ReactFlow UI] User > {user_input}")

    async def stream_generator():
        inputs = {"messages": [HumanMessage(content=user_input)]}
        from config import stream_queue
        
        # Vercel AI SDK í˜¸í™˜ Data Stream Chunk ìƒì„± í•¨ìˆ˜
        def make_text_chunk(text: str):
            return f'0:{json.dumps(text, ensure_ascii=False)}\n'
            
        def make_data_status(node_id: str, status: str, node_type: str = "agent", error: str = None):
            data_obj = {
                "type": "data-node-execution-status",
                "data": {
                    "nodeId": node_id,
                    "nodeType": node_type,
                    "status": status,
                }
            }
            if error:
                data_obj["data"]["error"] = error
            return f'8:[{json.dumps(data_obj, ensure_ascii=False)}]\n'
        
        import asyncio
        graph_task = None
        
        async def run_graph():
            try:
                # ì´ˆê¸° ìƒíƒœ: Router ì‹œì‘
                await stream_queue.put(make_data_status("start", "success"))
                await stream_queue.put(make_data_status("router", "running"))
                
                async for event in agent_app.astream(inputs):
                    for key, value in event.items():
                        if key == "router":
                            await stream_queue.put(make_data_status("router", "success"))
                        elif key == "orchestrator":
                            await stream_queue.put(make_data_status("orchestrator", "running"))
                        elif key == "workers":
                            # ê°œë³„ ì´ë²¤íŠ¸(LogSpecialist ë“±)ê°€ EVENT íë¡œ ë“¤ì–´ì˜¤ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì „ì—­ ìƒíƒœë§Œ ê´€ë¦¬
                            await stream_queue.put(make_data_status("orchestrator", "success"))
                        elif key == "synthesizer":
                            await stream_queue.put(make_data_status("synthesizer", "running"))
                        elif key == "simple_agent":
                            await stream_queue.put(make_data_status("router", "success"))
                            await stream_queue.put(make_data_status("simple_agent", "running"))
                            msg = value["messages"][-1].content
                            await stream_queue.put(make_data_status("simple_agent", "success"))
                            await stream_queue.put(make_data_status("end", "success"))
                            await stream_queue.put(make_text_chunk(msg))
                            
            except Exception as e:
                logger.error(f"âŒ [Graph] ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                await stream_queue.put(make_data_status("router", "error", error=str(e)))
            finally:
                await stream_queue.put("EOF")

        graph_task = asyncio.create_task(run_graph())
        import time
        token_buffer = ""
        last_flush = time.time()

        while True:
            if await request.is_disconnected():
                logger.warning("âš ï¸ [API] í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€")
                if graph_task and not graph_task.done():
                    graph_task.cancel()
                break

            try:
                msg = await asyncio.wait_for(stream_queue.get(), timeout=2.0)
            except asyncio.TimeoutError:
                if token_buffer:
                    yield make_text_chunk(token_buffer)
                    token_buffer = ""
                    last_flush = time.time()
                yield " \n"
                continue

            if msg == "EOF":
                if token_buffer:
                    yield make_text_chunk(token_buffer)
                yield 'd:{"finishReason":"stop"}\n'
                break
                
            elif msg.startswith("0:") or msg.startswith("8:"):
                yield msg
                
            elif msg.startswith("TOKEN:"):
                token_buffer += msg.replace("TOKEN:", "", 1)
                now = time.time()
                if now - last_flush >= 0.1:
                    yield make_text_chunk(token_buffer)
                    token_buffer = ""
                    last_flush = now
                    
            elif msg.startswith("FINAL:"):
                text = msg.replace("FINAL:", "", 1)
                yield make_data_status("agent", "success")
                yield make_data_status("end", "success")
                yield make_text_chunk(text)

            else:
                if msg.startswith("EVENT:"):
                    action_text = msg.replace("EVENT:", "", 1).strip()
                    
                    target_node = "tools" # Default
                    if "LogSpecialist" in action_text:
                        target_node = "worker_log"
                    elif "MetricSpecialist" in action_text:
                        target_node = "worker_metric"
                    elif "K8sSpecialist" in action_text:
                        target_node = "worker_k8s"
                    elif "synthesizer" in action_text.lower():
                        target_node = "synthesizer"
                        
                    # Tool completion usually has 'ì™„ë£Œ' or 'ê²°ê³¼' in Korean logs
                    if "ì™„ë£Œ" in action_text or "ê²°ê³¼" in action_text:
                        yield make_data_status(target_node, "success")
                        if target_node == "synthesizer":
                            yield make_data_status("end", "success")
                    else:
                        yield make_data_status(target_node, "running")
                    
                    yield make_text_chunk(f"[ê³¼ì •] {action_text}\n")

    return StreamingResponse(
        stream_generator(), 
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

# ========================================================
# OpenWebUI ì—°ë™ì„ ìœ„í•œ OpenAI í˜¸í™˜ API (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
# ========================================================
@app.post("/v1/chat/completions")
async def openai_compatible_endpoint(request: Request):
    """OpenWebUI ë“± OpenAI ê·œê²©ì„ ìš”êµ¬í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸ë¥¼ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸"""
    data = await request.json()
    
    # messages ë°°ì—´ì—ì„œ ë§ˆì§€ë§‰ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì¶”ì¶œ
    messages = data.get("messages", [])
    user_input = messages[-1]["content"] if messages else ""
    model_name = data.get("model", "qwen-k8s-agent")
    
    logger.info(f"[OpenWebUI] User > {user_input}")

    async def stream_generator():
        inputs = {"messages": [HumanMessage(content=user_input)]}
        from config import stream_queue
        
        # ë‚´ë¶€ ì§„í–‰ ìƒí™©ì„ OpenWebUIì—ë„ ë³´ì—¬ì£¼ê¸° ìœ„í•œ í—¬í¼ í•¨ìˆ˜
        def make_chunk(text):
            chunk = {
                "id": "chatcmpl-123",
                "object": "chat.completion.chunk",
                "model": model_name,
                "choices": [{"index": 0, "delta": {"content": text}, "finish_reason": None}]
            }
            return f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
        
        # OpenWebUI í˜¸í™˜ì„±ì„ ìœ„í•œ "ë‹¨ì¼ Think Block" ì „ì†¡ í—¬í¼
        # ì—¬ëŸ¬ ë²ˆ ì—´ê³  ë‹«ìœ¼ë©´ ë Œë”ëŸ¬ì— ì‹¬í•œ ë ‰ì´ ê±¸ë¦¬ë¯€ë¡œ, í•œ ë²ˆë§Œ ì—´ê³  ë‚´ë¶€ì—ì„œ ì¤„ë°”ê¿ˆì„ í†µí•´ ì¶”ê°€í•©ë‹ˆë‹¤.
        
        # --- ë³µí•© ìŠ¤íŠ¸ë¦¬ë° ë¡œì§ ---
        # 1. LangGraphì˜ astream ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼
        # 2. ë°±ê·¸ë¼ìš´ë“œ Workerì˜ ì§„í–‰ ìƒíƒœë¥¼ ë‹´ëŠ” stream_queue 
        
        import asyncio
        queue_task = None
        graph_task = None
        
        async def run_graph():
            try:
                async for event in agent_app.astream(inputs):
                    for key, value in event.items():
                        if key == "router":
                            await stream_queue.put("EVENT:ğŸ”„ `[System]` ë¼ìš°í„° ëª¨ë“œ ê²°ì • ì¤‘...")
                        elif key == "orchestrator":
                            await stream_queue.put("EVENT:ğŸ“‹ `[System]` ì‘ì—… ê³„íš ìˆ˜ë¦½ ì¤‘...")
                        elif key == "workers":
                            results = value.get("worker_results", [])
                            await stream_queue.put(f"EVENT:ğŸ‘· `[System]` {len(results)}ê°œ ë³‘ë ¬ ì‘ì—… ì‹¤í–‰ ì™„ë£Œ.")
                        elif key == "synthesizer":
                            pass
                        elif key == "simple_agent":
                            msg = value["messages"][-1].content
                            await stream_queue.put(f"FINAL:{msg}")
            except Exception as e:
                logger.error(f"âŒ [Graph] ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                # ì—ëŸ¬ ë°œìƒ ì‹œ UIì— ëª…ì‹œì ìœ¼ë¡œ ì•Œë¦¼
                await stream_queue.put(f"FINAL:\n\nâš ï¸ **ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤:**\n```\n{str(e)}\n```")
            finally:
                # ì •ìƒ/ë¹„ì •ìƒ ì¢…ë£Œ ìƒê´€ì—†ì´ ë°˜ë“œì‹œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹œê·¸ë„ ì „ì†¡
                await stream_queue.put("EOF")

        graph_task = asyncio.create_task(run_graph())
        
        has_started_thinking = False
        has_finished_thinking = False
        import time
        token_buffer = ""
        last_flush_time = time.time()

        while True:
            # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€(ìƒˆë¡œê³ ì¹¨, ì¤‘ì§€ë²„íŠ¼, íƒ€ì„ì•„ì›ƒ ì¬ì‹œë„ ë“±) ê°ì§€
            if await request.is_disconnected():
                logger.warning("âš ï¸ [API] í´ë¼ì´ì–¸íŠ¸(OpenWebUI) ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                if graph_task and not graph_task.done():
                    graph_task.cancel()
                break

            try:
                # ì”ì—¬ í† í°ì´ ìˆìœ¼ë©´ ì§§ê²Œ ëŒ€ê¸°, ì—†ìœ¼ë©´ 5ì´ˆ ëŒ€ê¸°(Keep-Aliveìš©)
                timeout_val = 0.05 if token_buffer else 5.0
                msg = await asyncio.wait_for(stream_queue.get(), timeout=timeout_val)
            except asyncio.TimeoutError:
                if token_buffer:
                    yield make_chunk(token_buffer)
                    token_buffer = ""
                    last_flush_time = time.time()
                else:
                    yield ": keep-alive\n\n"
                continue

            if msg == "EOF":
                if token_buffer:
                    yield make_chunk(token_buffer)
                    token_buffer = ""
                if has_started_thinking and not has_finished_thinking:
                    yield make_chunk("\n</think>\n\n")
                break
                
            elif msg.startswith("EVENT:"):
                if token_buffer:
                    yield make_chunk(token_buffer)
                    token_buffer = ""
                    
                text = msg.replace("EVENT:", "", 1)
                if not has_started_thinking:
                    yield make_chunk("<think>\n" + text + "\n")
                    has_started_thinking = True
                else:
                    yield make_chunk(text + "\n")
                    
            elif msg.startswith("TOKEN:"):
                if has_started_thinking and not has_finished_thinking:
                    yield make_chunk("\n</think>\n\n")
                    has_finished_thinking = True
                
                # ë¸Œë¼ìš°ì € UI ë ‰(Lag)ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ í† í°ì„ ëª¨ìë‹ˆë‹¤.
                token_buffer += msg.replace("TOKEN:", "", 1)
                now = time.time()
                # 0.05ì´ˆ(ì´ˆë‹¹ 20í”„ë ˆì„) ê°„ê²©ìœ¼ë¡œ ëª¨ì•„ì„œ í™”ë©´ì— ì†¡ì¶œí•©ë‹ˆë‹¤.
                if now - last_flush_time >= 0.05:
                    yield make_chunk(token_buffer)
                    token_buffer = ""
                    last_flush_time = now
                
            elif msg.startswith("FINAL:"):
                if token_buffer:
                    yield make_chunk(token_buffer)
                    token_buffer = ""
                    
                if has_started_thinking and not has_finished_thinking:
                    yield make_chunk("\n</think>\n\n")
                    has_finished_thinking = True
                    
                yield make_chunk(msg.replace("FINAL:", "", 1))
                
            else:
                if token_buffer:
                    yield make_chunk(token_buffer)
                    token_buffer = ""
                    
                if not has_started_thinking:
                    yield make_chunk("<think>\n" + msg + "\n")
                    has_started_thinking = True
                else:
                    yield make_chunk(msg + "\n")
                
        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ
        end_chunk = {
            "id": "chatcmpl-123",
            "object": "chat.completion.chunk",
            "model": model_name,
            "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]
        }
        yield f"data: {json.dumps(end_chunk, ensure_ascii=False)}\n\n"
        yield "data: [DONE]\n\n"

    return StreamingResponse(
        stream_generator(), 
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no" # Nginx/K8s Ingressì˜ SSE ë²„í¼ë§ ê°•ì œ ë¹„í™œì„±í™”
        }
    )

if __name__ == "__main__":
    uvicorn.run("api_server:app", host="0.0.0.0", port=8000, reload=True)
